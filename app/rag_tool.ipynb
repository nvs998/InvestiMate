{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "import uuid\n",
    "\n",
    "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embedder = SentenceTransformer(EMBED_MODEL)\n",
    "\n",
    "# Persistent local vector DB folder\n",
    "chroma = chromadb.PersistentClient(path=\"./vecdb\")\n",
    "\n",
    "# Create or get a collection (think “index”)\n",
    "collection = chroma.get_or_create_collection(\n",
    "    name=\"docs\",\n",
    "    metadata={\"hnsw:space\": \"cosine\"},  # cosine similarity (works well for MiniLM)\n",
    "    embedding_function=embedding_functions.SentenceTransformerEmbeddingFunction(EMBED_MODEL)\n",
    ")\n",
    "\n",
    "def chunk(text, title, doc_id, source, chunk_tokens=450, overlap=80):\n",
    "    \"\"\"\n",
    "    Lightweight splitter: slices text into overlapping windows.\n",
    "    Tip: swap for tiktoken or a sentence-aware splitter for cleaner boundaries.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    step = chunk_tokens - overlap\n",
    "    out, i = [], 0\n",
    "    while i < len(words):\n",
    "        piece = \" \".join(words[i:i+chunk_tokens])\n",
    "        out.append({\n",
    "            \"id\": f\"{doc_id}:{i}\",     # globally unique chunk id\n",
    "            \"text\": piece,\n",
    "            \"metadata\": {\n",
    "                \"doc_id\": doc_id,      # tie chunks back to the doc\n",
    "                \"title\": title,\n",
    "                \"source\": source,\n",
    "                \"chunk_index\": i,\n",
    "            }\n",
    "        })\n",
    "        i += step\n",
    "    return out\n",
    "\n",
    "def index_document(raw_text: str, title: str, source: str):\n",
    "    \"\"\"\n",
    "    Splits -> embeds -> stores.\n",
    "    Returns a doc_id so you can reference or re-index later.\n",
    "    \"\"\"\n",
    "    doc_id = str(uuid.uuid4())\n",
    "    chunks = chunk(raw_text, title, doc_id, source)\n",
    "    collection.add(\n",
    "        ids=[c[\"id\"] for c in chunks],\n",
    "        documents=[c[\"text\"] for c in chunks],\n",
    "        metadatas=[c[\"metadata\"] for c in chunks],\n",
    "    )\n",
    "    return doc_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
